{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hindi8.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOl+7tSf3XBUFb+tE2fLJkt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ayushmabahuguna/github_series/blob/master/Hindi8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmHbbnuHREcc"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from string import digits\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "%matplotlib inline\r\n",
        "import re\r\n",
        "import logging\r\n",
        "import os\r\n",
        "import tensorflow as tf\r\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\r\n",
        "logging.getLogger('tensorflow').setLevel(logging.FATAL)\r\n",
        "import matplotlib.ticker as ticker\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "import unicodedata\r\n",
        "import io\r\n",
        "import time\r\n",
        "import warnings\r\n",
        "import sys\r\n",
        "\r\n",
        "\r\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\r\n",
        "    for filename in filenames:\r\n",
        "        print(os.path.join(dirname, filename))\r\n",
        "        \r\n",
        "PATH = \"/content/Hindi_English_Truncated_Corpus.csv\""
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmBwRWI4VtDn",
        "outputId": "e38b785a-6254-4c84-bfa3-e37fac559a7d"
      },
      "source": [
        "!pip install -U tensorflow"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: tensorflow in /usr/local/lib/python3.6/dist-packages (2.4.0)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: termcolor~=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: absl-py~=0.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.5.0,>=2.4.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum~=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: h5py~=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: flatbuffers~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied, skipping upgrade: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied, skipping upgrade: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.36.2)\n",
            "Requirement already satisfied, skipping upgrade: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions~=3.7.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard~=2.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied, skipping upgrade: grpcio~=1.32.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.32.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy~=1.19.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: six~=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow) (51.3.3)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (0.4.2)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (1.7.0)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (3.3.3)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (1.17.2)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.2.0)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.6)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.4.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MF_QN6UXWW9Q"
      },
      "source": [
        "def unicode_to_ascii(s):\r\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\r\n",
        "        if unicodedata.category(c) != 'Mn')\r\n",
        "def preprocess_sentence(w):\r\n",
        "    w = unicode_to_ascii(w.lower().strip())\r\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\r\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\r\n",
        "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\r\n",
        "    w = w.rstrip().strip()\r\n",
        "    return w\r\n",
        "\r\n",
        "def hindi_preprocess_sentence(w):\r\n",
        "    w = w.rstrip().strip()\r\n",
        "    return w"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCrlsfVdW3JF"
      },
      "source": [
        "def create_dataset(path=PATH):\r\n",
        "    lines=pd.read_csv(path,encoding='ISO-8859-1')\r\n",
        "    lines=lines.dropna()\r\n",
        "    lines = lines[lines['source']=='ted']\r\n",
        "    en = []\r\n",
        "    hd = []\r\n",
        "    for i, j in zip(lines['english_sentence'], lines['hindi_sentence']):\r\n",
        "        en_1 = [preprocess_sentence(w) for w in i.split(' ')]\r\n",
        "        en_1.append('<end>')\r\n",
        "        en_1.insert(0, '<start>')\r\n",
        "        hd_1 = [hindi_preprocess_sentence(w) for w in j.split(' ')]\r\n",
        "        hd_1.append('<end>')\r\n",
        "        hd_1.insert(0, '<start>')\r\n",
        "        en.append(en_1)\r\n",
        "        hd.append(hd_1)\r\n",
        "    return hd, en"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_6PPkkiXhO0"
      },
      "source": [
        "def max_length(tensor):\r\n",
        "    return max(len(t) for t in tensor)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J027qf-BX1ps"
      },
      "source": [
        "def tokenize(lang):\r\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\r\n",
        "  lang_tokenizer.fit_on_texts(lang)\r\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\r\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,padding='post')\r\n",
        "  return tensor, lang_tokenizer"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmzpV1xeX5Wh"
      },
      "source": [
        "def load_dataset(path=PATH):\r\n",
        "    targ_lang, inp_lang = create_dataset(path)\r\n",
        "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\r\n",
        "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\r\n",
        "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlSXPVhbX909"
      },
      "source": [
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(PATH)\r\n",
        "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RU_bjP_rYCyu",
        "outputId": "2bb73185-8ce8-4125-b71f-6db6e04d914a"
      },
      "source": [
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\r\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "31904 31904 7977 7977\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOo7rgzXg2x5",
        "outputId": "bbdc858a-0993-4a69-cb99-675331f3f853"
      },
      "source": [
        "def convert(lang, tensor):\r\n",
        "  for t in tensor:\r\n",
        "    if t!=0:\r\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))\r\n",
        "    \r\n",
        "print (\"Input Language; index to word mapping\")\r\n",
        "convert(inp_lang, input_tensor_train[0])\r\n",
        "print ()\r\n",
        "print (\"Target Language; index to word mapping\")\r\n",
        "convert(targ_lang, target_tensor_train[0])"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Language; index to word mapping\n",
            "1 ----> <start>\n",
            "23 ----> but\n",
            "3 ----> the\n",
            "285 ----> point\n",
            "181 ----> is ,\n",
            "15 ----> it\n",
            "188 ----> doesn t\n",
            "209 ----> feel\n",
            "9 ----> that\n",
            "417 ----> way .\n",
            "2 ----> <end>\n",
            "\n",
            "Target Language; index to word mapping\n",
            "1 ----> <start>\n",
            "60 ----> à¤²à¥à¤à¤¿à¤¨\n",
            "946 ----> à¤®à¥à¤¦à¥à¤¦à¤¾\n",
            "12 ----> à¤¯à¤¹\n",
            "19 ----> à¤¹à¥,\n",
            "10 ----> à¤à¤¿\n",
            "79 ----> à¤à¤¸à¥\n",
            "80 ----> à¤à¤¸\n",
            "54 ----> à¤¤à¤°à¤¹\n",
            "7 ----> à¤¸à¥\n",
            "296 ----> à¤®à¤¹à¤¸à¥à¤¸\n",
            "15 ----> à¤¨à¤¹à¥à¤\n",
            "48 ----> à¤à¤°à¤¤à¥\n",
            "13 ----> à¤¹à¥à¤\n",
            "2 ----> <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcXrUFGmg699"
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\r\n",
        "BATCH_SIZE = 64\r\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\r\n",
        "embedding_dim = 128\r\n",
        "units = 256\r\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\r\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\r\n",
        "\r\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\r\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8cM57gbhBE1"
      },
      "source": [
        "class Encoder(tf.keras.Model):\r\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\r\n",
        "    super(Encoder, self).__init__()\r\n",
        "    self.batch_sz = batch_sz\r\n",
        "    self.enc_units = enc_units\r\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\r\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\r\n",
        "                                   return_sequences=True,\r\n",
        "                                   return_state=True,\r\n",
        "                                   recurrent_initializer='glorot_uniform')\r\n",
        "\r\n",
        "  def call(self, x, hidden):\r\n",
        "    x = self.embedding(x)\r\n",
        "    output, state = self.gru(x, initial_state = hidden)\r\n",
        "    return output, state\r\n",
        "\r\n",
        "  def initialize_hidden_state(self):\r\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))\r\n",
        "\r\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcsNu09uh_2l"
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\r\n",
        "  def __init__(self, units):\r\n",
        "    super(BahdanauAttention, self).__init__()\r\n",
        "    self.W1 = tf.keras.layers.Dense(units)\r\n",
        "    self.W2 = tf.keras.layers.Dense(units)\r\n",
        "    self.V = tf.keras.layers.Dense(1)\r\n",
        "\r\n",
        "  def call(self, query, values):\r\n",
        "    hidden_with_time_axis = tf.expand_dims(query, 1)\r\n",
        "    score = self.V(tf.nn.tanh(\r\n",
        "        self.W1(values) + self.W2(hidden_with_time_axis)))\r\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\r\n",
        "    context_vector = attention_weights * values\r\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\r\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lc52UrG0iFin"
      },
      "source": [
        "class Decoder(tf.keras.Model):\r\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\r\n",
        "    super(Decoder, self).__init__()\r\n",
        "    self.batch_sz = batch_sz\r\n",
        "    self.dec_units = dec_units\r\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\r\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\r\n",
        "                                   return_sequences=True,\r\n",
        "                                   return_state=True,\r\n",
        "                                   recurrent_initializer='glorot_uniform')\r\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\r\n",
        "    self.attention = BahdanauAttention(self.dec_units)\r\n",
        "\r\n",
        "  def call(self, x, hidden, enc_output):\r\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\r\n",
        "    x = self.embedding(x)\r\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\r\n",
        "    output, state = self.gru(x)\r\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\r\n",
        "    x = self.fc(output)\r\n",
        "    return x, state, attention_weights\r\n",
        "\r\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\r\n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaSrLYykiNXm"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\r\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\r\n",
        "    from_logits=True, reduction='none')\r\n",
        "\r\n",
        "def loss_function(real, pred):\r\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\r\n",
        "  loss_ = loss_object(real, pred)\r\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\r\n",
        "#   print(type(mask))\r\n",
        "  loss_ *= mask\r\n",
        "  return tf.reduce_mean(loss_)\r\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUpTAnGKiTCD"
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\r\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\r\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\r\n",
        "                                 encoder=encoder,\r\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3jGzTjFiaSX"
      },
      "source": [
        "@tf.function\r\n",
        "def train_step(inp, targ, enc_hidden):\r\n",
        "  loss = 0\r\n",
        "  with tf.GradientTape() as tape:\r\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\r\n",
        "    dec_hidden = enc_hidden\r\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\r\n",
        "    # Teacher forcing\r\n",
        "    for t in range(1, targ.shape[1]):\r\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\r\n",
        "      loss += loss_function(targ[:, t], predictions)\r\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\r\n",
        "\r\n",
        "  batch_loss = (loss / int(targ.shape[1]))\r\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\r\n",
        "  gradients = tape.gradient(loss, variables)\r\n",
        "  optimizer.apply_gradients(zip(gradients, variables))      \r\n",
        "  return batch_loss"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R31vpPXqieYB",
        "outputId": "93624205-3a47-4802-a49f-205b14c36909"
      },
      "source": [
        "EPOCHS = 30\r\n",
        "for epoch in range(EPOCHS):\r\n",
        "  start = time.time()\r\n",
        "  enc_hidden = encoder.initialize_hidden_state()\r\n",
        "  total_loss = 0\r\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\r\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\r\n",
        "    total_loss += batch_loss\r\n",
        "    if batch % 100 == 0:\r\n",
        "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\r\n",
        "                                                     batch,\r\n",
        "                                                     batch_loss.numpy()))\r\n",
        "  if (epoch + 1) % 2 == 0:\r\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\r\n",
        "\r\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\r\n",
        "                                      total_loss / steps_per_epoch))\r\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 2.7581\n",
            "Epoch 1 Batch 100 Loss 1.9370\n",
            "Epoch 1 Batch 200 Loss 1.7570\n",
            "Epoch 1 Batch 300 Loss 1.9108\n",
            "Epoch 1 Batch 400 Loss 1.8029\n",
            "Epoch 1 Loss 1.9432\n",
            "Time taken for 1 epoch 96.36759757995605 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.8038\n",
            "Epoch 2 Batch 100 Loss 1.8526\n",
            "Epoch 2 Batch 200 Loss 1.7498\n",
            "Epoch 2 Batch 300 Loss 1.7346\n",
            "Epoch 2 Batch 400 Loss 1.8171\n",
            "Epoch 2 Loss 1.7329\n",
            "Time taken for 1 epoch 70.0538763999939 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.5439\n",
            "Epoch 3 Batch 100 Loss 1.6562\n",
            "Epoch 3 Batch 200 Loss 1.5766\n",
            "Epoch 3 Batch 300 Loss 1.5738\n",
            "Epoch 3 Batch 400 Loss 1.6009\n",
            "Epoch 3 Loss 1.6252\n",
            "Time taken for 1 epoch 70.04120349884033 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.6037\n",
            "Epoch 4 Batch 100 Loss 1.5967\n",
            "Epoch 4 Batch 200 Loss 1.4759\n",
            "Epoch 4 Batch 300 Loss 1.4641\n",
            "Epoch 4 Batch 400 Loss 1.5321\n",
            "Epoch 4 Loss 1.5411\n",
            "Time taken for 1 epoch 70.44517707824707 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.5210\n",
            "Epoch 5 Batch 100 Loss 1.3078\n",
            "Epoch 5 Batch 200 Loss 1.3876\n",
            "Epoch 5 Batch 300 Loss 1.5540\n",
            "Epoch 5 Batch 400 Loss 1.4979\n",
            "Epoch 5 Loss 1.4577\n",
            "Time taken for 1 epoch 70.1725263595581 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.3505\n",
            "Epoch 6 Batch 100 Loss 1.2118\n",
            "Epoch 6 Batch 200 Loss 1.3830\n",
            "Epoch 6 Batch 300 Loss 1.3210\n",
            "Epoch 6 Batch 400 Loss 1.3422\n",
            "Epoch 6 Loss 1.3711\n",
            "Time taken for 1 epoch 70.89504933357239 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.2868\n",
            "Epoch 7 Batch 100 Loss 1.2467\n",
            "Epoch 7 Batch 200 Loss 1.3613\n",
            "Epoch 7 Batch 300 Loss 1.3312\n",
            "Epoch 7 Batch 400 Loss 1.2887\n",
            "Epoch 7 Loss 1.2869\n",
            "Time taken for 1 epoch 70.77353715896606 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 1.1224\n",
            "Epoch 8 Batch 100 Loss 1.1243\n",
            "Epoch 8 Batch 200 Loss 1.0902\n",
            "Epoch 8 Batch 300 Loss 1.2008\n",
            "Epoch 8 Batch 400 Loss 1.2711\n",
            "Epoch 8 Loss 1.2056\n",
            "Time taken for 1 epoch 70.78384256362915 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 1.0888\n",
            "Epoch 9 Batch 100 Loss 1.0034\n",
            "Epoch 9 Batch 200 Loss 1.0805\n",
            "Epoch 9 Batch 300 Loss 1.1417\n",
            "Epoch 9 Batch 400 Loss 1.0694\n",
            "Epoch 9 Loss 1.1274\n",
            "Time taken for 1 epoch 70.65820169448853 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 1.0200\n",
            "Epoch 10 Batch 100 Loss 0.9940\n",
            "Epoch 10 Batch 200 Loss 1.1074\n",
            "Epoch 10 Batch 300 Loss 1.1201\n",
            "Epoch 10 Batch 400 Loss 1.1147\n",
            "Epoch 10 Loss 1.0512\n",
            "Time taken for 1 epoch 70.86097192764282 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.9922\n",
            "Epoch 11 Batch 100 Loss 0.9274\n",
            "Epoch 11 Batch 200 Loss 1.0618\n",
            "Epoch 11 Batch 300 Loss 1.0144\n",
            "Epoch 11 Batch 400 Loss 0.9869\n",
            "Epoch 11 Loss 0.9800\n",
            "Time taken for 1 epoch 71.11301851272583 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.8053\n",
            "Epoch 12 Batch 100 Loss 0.9747\n",
            "Epoch 12 Batch 200 Loss 0.7723\n",
            "Epoch 12 Batch 300 Loss 0.8377\n",
            "Epoch 12 Batch 400 Loss 0.9486\n",
            "Epoch 12 Loss 0.9112\n",
            "Time taken for 1 epoch 70.85208702087402 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.7761\n",
            "Epoch 13 Batch 100 Loss 0.8323\n",
            "Epoch 13 Batch 200 Loss 0.8237\n",
            "Epoch 13 Batch 300 Loss 0.9522\n",
            "Epoch 13 Batch 400 Loss 0.9035\n",
            "Epoch 13 Loss 0.8425\n",
            "Time taken for 1 epoch 70.30508065223694 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.7283\n",
            "Epoch 14 Batch 100 Loss 0.7387\n",
            "Epoch 14 Batch 200 Loss 0.8431\n",
            "Epoch 14 Batch 300 Loss 0.7912\n",
            "Epoch 14 Batch 400 Loss 0.7820\n",
            "Epoch 14 Loss 0.7823\n",
            "Time taken for 1 epoch 70.36162543296814 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.8338\n",
            "Epoch 15 Batch 100 Loss 0.7430\n",
            "Epoch 15 Batch 200 Loss 0.7349\n",
            "Epoch 15 Batch 300 Loss 0.6853\n",
            "Epoch 15 Batch 400 Loss 0.7529\n",
            "Epoch 15 Loss 0.7274\n",
            "Time taken for 1 epoch 70.42857336997986 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.6544\n",
            "Epoch 16 Batch 100 Loss 0.6171\n",
            "Epoch 16 Batch 200 Loss 0.5898\n",
            "Epoch 16 Batch 300 Loss 0.6405\n",
            "Epoch 16 Batch 400 Loss 0.6525\n",
            "Epoch 16 Loss 0.6741\n",
            "Time taken for 1 epoch 70.80796790122986 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.5680\n",
            "Epoch 17 Batch 100 Loss 0.6521\n",
            "Epoch 17 Batch 200 Loss 0.6549\n",
            "Epoch 17 Batch 300 Loss 0.6264\n",
            "Epoch 17 Batch 400 Loss 0.6638\n",
            "Epoch 17 Loss 0.6269\n",
            "Time taken for 1 epoch 70.67508959770203 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.5176\n",
            "Epoch 18 Batch 100 Loss 0.6654\n",
            "Epoch 18 Batch 200 Loss 0.5969\n",
            "Epoch 18 Batch 300 Loss 0.5461\n",
            "Epoch 18 Batch 400 Loss 0.5820\n",
            "Epoch 18 Loss 0.5835\n",
            "Time taken for 1 epoch 71.35335397720337 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.6271\n",
            "Epoch 19 Batch 100 Loss 0.5719\n",
            "Epoch 19 Batch 200 Loss 0.4298\n",
            "Epoch 19 Batch 300 Loss 0.5642\n",
            "Epoch 19 Batch 400 Loss 0.5425\n",
            "Epoch 19 Loss 0.5446\n",
            "Time taken for 1 epoch 71.03440308570862 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.4805\n",
            "Epoch 20 Batch 100 Loss 0.5044\n",
            "Epoch 20 Batch 200 Loss 0.4764\n",
            "Epoch 20 Batch 300 Loss 0.5906\n",
            "Epoch 20 Batch 400 Loss 0.5878\n",
            "Epoch 20 Loss 0.5092\n",
            "Time taken for 1 epoch 70.58577370643616 sec\n",
            "\n",
            "Epoch 21 Batch 0 Loss 0.4200\n",
            "Epoch 21 Batch 100 Loss 0.5109\n",
            "Epoch 21 Batch 200 Loss 0.5148\n",
            "Epoch 21 Batch 300 Loss 0.5501\n",
            "Epoch 21 Batch 400 Loss 0.4878\n",
            "Epoch 21 Loss 0.4728\n",
            "Time taken for 1 epoch 70.7431092262268 sec\n",
            "\n",
            "Epoch 22 Batch 0 Loss 0.4240\n",
            "Epoch 22 Batch 100 Loss 0.4533\n",
            "Epoch 22 Batch 200 Loss 0.5689\n",
            "Epoch 22 Batch 300 Loss 0.4104\n",
            "Epoch 22 Batch 400 Loss 0.4806\n",
            "Epoch 22 Loss 0.4441\n",
            "Time taken for 1 epoch 71.00041437149048 sec\n",
            "\n",
            "Epoch 23 Batch 0 Loss 0.3500\n",
            "Epoch 23 Batch 100 Loss 0.4431\n",
            "Epoch 23 Batch 200 Loss 0.3472\n",
            "Epoch 23 Batch 300 Loss 0.4852\n",
            "Epoch 23 Batch 400 Loss 0.4707\n",
            "Epoch 23 Loss 0.4128\n",
            "Time taken for 1 epoch 70.58803725242615 sec\n",
            "\n",
            "Epoch 24 Batch 0 Loss 0.3642\n",
            "Epoch 24 Batch 100 Loss 0.3019\n",
            "Epoch 24 Batch 200 Loss 0.3759\n",
            "Epoch 24 Batch 300 Loss 0.3571\n",
            "Epoch 24 Batch 400 Loss 0.3521\n",
            "Epoch 24 Loss 0.3837\n",
            "Time taken for 1 epoch 70.87386274337769 sec\n",
            "\n",
            "Epoch 25 Batch 0 Loss 0.3073\n",
            "Epoch 25 Batch 100 Loss 0.3378\n",
            "Epoch 25 Batch 200 Loss 0.3937\n",
            "Epoch 25 Batch 300 Loss 0.3251\n",
            "Epoch 25 Batch 400 Loss 0.3902\n",
            "Epoch 25 Loss 0.3577\n",
            "Time taken for 1 epoch 70.71940422058105 sec\n",
            "\n",
            "Epoch 26 Batch 0 Loss 0.2672\n",
            "Epoch 26 Batch 100 Loss 0.2635\n",
            "Epoch 26 Batch 200 Loss 0.3269\n",
            "Epoch 26 Batch 300 Loss 0.3100\n",
            "Epoch 26 Batch 400 Loss 0.3815\n",
            "Epoch 26 Loss 0.3347\n",
            "Time taken for 1 epoch 70.90424942970276 sec\n",
            "\n",
            "Epoch 27 Batch 0 Loss 0.2917\n",
            "Epoch 27 Batch 100 Loss 0.3102\n",
            "Epoch 27 Batch 200 Loss 0.3024\n",
            "Epoch 27 Batch 300 Loss 0.3258\n",
            "Epoch 27 Batch 400 Loss 0.3388\n",
            "Epoch 27 Loss 0.3131\n",
            "Time taken for 1 epoch 70.96952438354492 sec\n",
            "\n",
            "Epoch 28 Batch 0 Loss 0.2278\n",
            "Epoch 28 Batch 100 Loss 0.2765\n",
            "Epoch 28 Batch 200 Loss 0.2753\n",
            "Epoch 28 Batch 300 Loss 0.2481\n",
            "Epoch 28 Batch 400 Loss 0.3132\n",
            "Epoch 28 Loss 0.2945\n",
            "Time taken for 1 epoch 71.15440082550049 sec\n",
            "\n",
            "Epoch 29 Batch 0 Loss 0.2153\n",
            "Epoch 29 Batch 100 Loss 0.2857\n",
            "Epoch 29 Batch 200 Loss 0.2366\n",
            "Epoch 29 Batch 300 Loss 0.3247\n",
            "Epoch 29 Batch 400 Loss 0.2333\n",
            "Epoch 29 Loss 0.2752\n",
            "Time taken for 1 epoch 71.50610518455505 sec\n",
            "\n",
            "Epoch 30 Batch 0 Loss 0.2322\n",
            "Epoch 30 Batch 100 Loss 0.2414\n",
            "Epoch 30 Batch 200 Loss 0.2833\n",
            "Epoch 30 Batch 300 Loss 0.2812\n",
            "Epoch 30 Batch 400 Loss 0.2251\n",
            "Epoch 30 Loss 0.2584\n",
            "Time taken for 1 epoch 71.3119387626648 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QP_DVz-rikU3"
      },
      "source": [
        "  def evaluate(sentence):\r\n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp))\r\n",
        "    sentence = preprocess_sentence(sentence)\r\n",
        "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\r\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\r\n",
        "                                                           maxlen=max_length_inp,\r\n",
        "                                                           padding='post')\r\n",
        "    inputs = tf.convert_to_tensor(inputs)\r\n",
        "    result = ''\r\n",
        "    hidden = [tf.zeros((1, units))]\r\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\r\n",
        "    dec_hidden = enc_hidden\r\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\r\n",
        "    for t in range(max_length_targ):\r\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input,\r\n",
        "                                                             dec_hidden,\r\n",
        "                                                             enc_out)\r\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\r\n",
        "        result += targ_lang.index_word[predicted_id] + ' '\r\n",
        "        if targ_lang.index_word[predicted_id] == '<end>':\r\n",
        "            return result, sentence\r\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\r\n",
        "    return result, sentence"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vhUAR1Jq9LF"
      },
      "source": [
        "def translate(sentence):\r\n",
        "    result, sentence = evaluate(sentence)\r\n",
        "    print('Input: %s' % (sentence))\r\n",
        "    print('Predicted translation: {}'.format(result))\r\n",
        "    "
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIA7DPQarCrV",
        "outputId": "51813fbb-9d65-415a-fafc-197858c0c564"
      },
      "source": [
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f743d4241d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eml-0gH9rGFf",
        "outputId": "b776abee-a02b-4987-a453-af789005418d"
      },
      "source": [
        "translate(u'politicians do not have permission to do what needs to be done.')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: politicians do not have permission to do what needs to be done .\n",
            "Predicted translation: à¤à¤° à¤¯à¥ à¤®à¤¤ à¤ªà¥à¤°à¥ à¤¤à¤°à¤¹ à¤à¥ à¤²à¤¿à¤ à¤¨à¤¹à¥à¤ à¤¦à¥ à¤¸à¤à¤¤à¥ à¥¤ <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJXlDv78rKLl",
        "outputId": "2b9a48d6-e29a-494d-e5f5-64984d4ed49a"
      },
      "source": [
        "translate(\"I love my India \")"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: i love my india\n",
            "Predicted translation: à¤®à¥à¤à¥ à¤¯à¥ à¤­à¤¾à¤°à¤¤ à¤¸à¥ <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgLfUH23rYTD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}